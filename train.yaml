!obj:pylearn2.train.Train {
    dataset: &train !obj:dataset.KBCDataset {
        which_set: 'train',
        home_dir: &data_dir '%(data_dir)s',
        max_labels: &max_labels 40,
    },
    model: !obj:pylearn2.models.mlp.MLP {
        layers: [
            !obj:pylearn2.sandbox.nlp.models.mlp.ProjectionLayer {
                layer_name: 'projection',
                dim: 10,
                irange: 0.3
            },
            !obj:pylearn2.models.mlp.RectifiedLinear {
                layer_name: 'h',
                dim: 10,
                irange: 0.3,
            },
            !obj:pylearn2.models.mlp.Softmax {
                layer_name: 'y',
                n_classes: 2,
                irange: 0.05,
                binary_target_dim: 1,
            },
        ],
        input_space: !obj:pylearn2.space.IndexSpace {
            dim: 3,
            max_labels: *max_labels,
        }
    },
    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
        train_iteration_mode: 'random_uniform',
        batches_per_iter: 3,
        batch_size: 100,
        # without monitoring_batch_size here, pylearn2 will copy both
        # monitoring_batch_size and monitoring_batches from their training 
        # counterparts which create an error
        monitoring_batch_size: 100, 
        learning_rate: .1,
        seed: 20141117,
        monitoring_dataset: {
            'train' : *train,
            'valid' : !obj:kbcomplete.dataset.KBCDataset {
                which_set: 'valid',
                home_dir: *data_dir,
                max_labels: *max_labels,
            },
            'test' : !obj:kbcomplete.dataset.KBCDataset {
                which_set: 'test',
                home_dir: *data_dir,
                max_labels: *max_labels,
            },
        },
        termination_criterion: !obj:pylearn2.termination_criteria.MonitorBased {
            channel_name: 'valid_objective',
            prop_decrease: 0.001,
            N: 50, # small epochs permit more waste
        },
#        learning_rule: !obj:pylearn2.training_algorithms.learning_rule.Momentum {
#            init_momentum: .5
#        },
        cost: !obj:pylearn2.costs.cost.SumOfCosts {
            costs: [
                !obj:pylearn2.costs.mlp.Default {},
                !obj:pylearn2.costs.mlp.WeightDecay {
                    coeffs: [ .0001, .0001, .0001, ]
                }
            ]
        },
    },
    extensions: [
        !obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest {
             # Use negative log likelihood instead of misclassification rate
             # because: (i) it is more sensitive and (ii) we care about
             # entity vectors but not classification 
             channel_name: 'valid_y_nll',
             save_path: '%(output_path)s',
        },
    ],
}