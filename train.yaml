!obj:pylearn2.train.Train {
    dataset: &train !obj:kbcomplete.dataset.BabelNetDataset {
        which_set: 'train',
        home_dir: &data_dir '%(data_dir)s',
        max_labels: &max_labels 40,
    },
    model: !obj:pylearn2.models.mlp.MLP {
        layers: [
            !obj:pylearn2.sandbox.nlp.models.mlp.ProjectionLayer {
                layer_name: 'projection',
                dim: 10,
                irange: 0.3
            },
#            !obj:pylearn2.models.mlp.Sigmoid {
#                layer_name: 'h',
#                dim: 10,
#                irange: 0.3,
#            },
#            !obj:pylearn2.models.lwta.LWTA {
#                layer_name: 'h',
#                dim: 10,
#                block_size: 2,
#                irange: 0.3,
#            },
            !obj:kbcomplete.models.attractor.Attractor {
                 block_size: 12,
                 partition_size: 10, 
                 init_scaler: 0.2,
                 layer_name: 'h0',
                 dim: 480,
                 irange : 0.05,
                 #soft: True,
                 rectified: True,
             },
            !obj:kbcomplete.models.attractor.Attractor {
                 block_size: 12,
                 partition_size: 10, 
                 init_scaler: 0.2,
                 layer_name: 'h1',
                 dim: 480,
                 irange : 0.05,
                 #soft: True,
                 rectified: True,
             },
#            !obj:pylearn2.models.mlp.Sigmoid {
#                layer_name: 'h1',
#                dim: 50,
#                irange: 0.3,
#            },
#            !obj:kbcomplete.models.attractor.Attractor {
#                 block_size: 12,
#                 partition_size: 10, 
#                 init_scaler: 0.2,
#                 layer_name: 'h2',
#                 dim: 480,
#                 irange : 0.05,
#                 #soft: True,
#                 rectified: True,
#             },
            !obj:pylearn2.models.mlp.Softmax {
                layer_name: 'y',
                n_classes: 2,
                irange: 0.3,
                binary_target_dim: 1,
            }
        ],
        input_space: !obj:pylearn2.space.IndexSpace {
            dim: 3,
            max_labels: *max_labels,
        }
    },
    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
        train_iteration_mode: 'random_uniform',
        batches_per_iter: 3,
        batch_size: 100,
        # without monitoring_batch_size here, pylearn2 will copy both
        # monitoring_batch_size and monitoring_batches from their training 
        # counterparts which create an error
        monitoring_batch_size: 100, 
        learning_rate: .1,
        #seed: 20141117,
        monitoring_dataset: {
            'train' : *train,
            'valid' : !obj:kbcomplete.dataset.BabelNetDataset {
                which_set: 'valid',
                home_dir: *data_dir,
                max_labels: *max_labels,
            },
            'test' : !obj:kbcomplete.dataset.BabelNetDataset {
                which_set: 'test',
                home_dir: *data_dir,
                max_labels: *max_labels,
            },
        },
        termination_criterion: !obj:pylearn2.termination_criteria.MonitorBased {
            channel_name: 'valid_objective',
            prop_decrease: 0.001,
            N: 50, # small epochs permit more waste
        },
#        learning_rule: !obj:pylearn2.training_algorithms.learning_rule.Momentum {
#            init_momentum: .5
#        },
        cost: !obj:pylearn2.costs.cost.SumOfCosts {
            costs: [
                !obj:pylearn2.costs.mlp.Default {},
                !obj:pylearn2.costs.mlp.WeightDecay {
                    coeffs: [ .0001, .0001, .0001, 
                        .0001, 
#                        .0001, 
                    ]
                }
            ]
        },
    },
    extensions: [
        !obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest {
             # Use negative log likelihood instead of misclassification rate
             # because: (i) it is more sensitive and (ii) we care about
             # entity vectors but not classification 
             channel_name: 'valid_y_nll',
             save_path: '%(output_path)s',
        },
        !obj:pylearn2.training_algorithms.sgd.MonitorBasedLRAdjuster {
             channel_name: 'train_objective',
             grow_amt: 1.1,
             shrink_amt: 0.9
        },
        !obj:kbcomplete.models.attractor.ScalerAdjustor {
            final_scaler: 0.01, 
            start: 10, 
            saturate: 50,
        }
    ],
}